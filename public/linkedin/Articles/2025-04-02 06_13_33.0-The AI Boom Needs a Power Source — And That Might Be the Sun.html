<html>
<head>
  <title>The AI Boom Needs a Power Source â€” And That Might Be the Sun</title>
  <style>
    body {
      margin: 0 auto;
      width: 744px;
      font-family: Source Serif Pro, serif;
      line-height: 32px;
      font-weight: 400;
      color: rgba(0, 0, 0, 0.7);
      font-size: 21px;
    }
    h1, h2, h3 {
      font-family: Source Sans Pro, Helvetica, Arial, sans-serif;
    }
    h1 a, h1 a:visited {
      color: inherit;
      text-decoration: none;
    }
    h1 {
      line-height: 48px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 42px;
      margin: 32px 0 20px;
    }
    h2 {
      line-height: 32px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 26px;
      margin: 28px 0;
    }
    h3 {
      line-height: 28px;
      font-weight: 600;
      color: rgba(0, 0, 0, 0.85);
      font-size: 21px;
      margin: 24px 0;
    }
    p {
      margin: 32px 0;
    }
    .created, .published {
      color: rgba(0, 0, 0, 0.55);
      font-size: 15px;
      line-height: 15px;
      margin: 20px 0;
    }
    .created + .published {
      margin-top: -12px;
    }
    blockquote {
      font-family: Georgia, Source Serif Pro, serif;
      font-style: italic;
      font-size: 24px;
      line-height: 36px;
      margin: 48px 120px;
      text-align: center;
    }
    a {
      word-wrap: break-word;
      outline: none;
      text-decoration: none;
      background-color: transparent;
      border: 0;
      color: #008CC9;
    }
    a:hover {
      text-decoration: underline;
    }
    a:visited {
      color: #8C68CB;
    }
    .center {
      text-align: center;
    }
    iframe {
      display: block;
      margin: 44px auto;
    }
    *:not(pre) + pre, pre:first-of-type {
      margin-top: 32px;
      padding-top: 32px;
    }
    pre:only-of-type {
      margin: 32px 0;
      padding: 32px;
    }
    pre {
      background: #F3F6F8;
      overflow-x: auto;
      display: block;
      font-size: 13px;
      font-family: monospace;
      line-height: 13px;
      padding: 0 32px 32px;
      white-space: pre;
    }
    a.embedded {
      background: #F3F6F8;
      display: block;
      padding: 32px;
      margin: 32px 0;
    }
    img {
      height: auto;
      max-width: 100%;
    }
    .slate-image-embed__resize-full-width img {
      width: 100%;
    }
    .series-logo {
      width: 48px;
      height: 48px;
      box-sizing: border-box;
      background-clip: content-box;
      border: 4px solid transparent;
      border-radius: 6px;
      object-fit: scale-down;
      float: left;
    }
    .series-title {
      font-size: 16px;
      font-weight: 600;
      vertical-align: top;
    }
    .series-description {
      color: rgba(0,0,0,.6);
      font-weight: 400;
      font-size: 14px;
      line-height: 20px;
    }
    div {
      margin: 32px 0;
    }
  </style>
</head>
<body>
      <h1>The AI Boom Needs a Power Source â€” And That Might Be the Sun</h1>
    <p class="created">Created on 2025-04-02 06:13</p>
  <p class="published">Published on ---</p>
  <div><h3>NVIDIA, AMD, Intel and the Future of AI Compute in a Renewable World</h3><p>ğŸ” The Premise: A Collision Course Between AI and EnergyThe AI revolution is not just technologicalâ€”itâ€™s geopolitical, economic, and infrastructural. From our phones to factories, artificial intelligence is transforming everything. But as the AI models grow more complex, as ambitions shift from narrow models to Artificial General Intelligence (AGI) and eventually Artificial Superintelligence (ASI), one question looms larger than all others:</p><p>How will we power it all?</p><blockquote><p>ğŸ’¡ â€œAIâ€™s power needs are becoming a new industrial revolution â€” and the grid isnâ€™t ready.â€</p></blockquote><p>The sheer scale of compute required to train and operate modern AI models has ushered in a new era of energy demand. Training GPT-3 required about 1.3 GWh of electricity, and GPT-4 and similar large models require even more. Inference, especially at scale in cloud services, can easily exceed training energy demands.</p><p>To contextualize: the human brain consumes about 20 watts to function. An AI model emulating a fraction of human cognitive ability today may consume megawatts. Extrapolating this to AGI, or global-scale AI deployment, the numbers become staggering.</p><p>By 2027, AI data centers using NVIDIA chips could consume 85 terawatt-hours annually. Thatâ€™s equivalent to the power usage of an entire mid-sized European nation.</p><blockquote><p>âš ï¸ â€œAI needs more electricity than entire countriesâ€”and itâ€™s only getting started.â€</p></blockquote><p>Meanwhile, utility-scale infrastructure is under strain. Grid congestion in areas like the Netherlands, California, and parts of Asia has delayed or halted data center builds. The AI revolution is colliding with the hard limits of electricity infrastructure.</p><p>This has sparked a global rethink: what if AIâ€™s power demands could be met locally, renewably, and efficiently?</p><p>Thatâ€™s the promise of perovskite solar cell (PSC) technologyâ€”and itâ€™s why Japanâ€™s energy strategy may hold global implications.</p><h3>ğŸ“ˆ Stock Price Forecasts: NVIDIA, AMD, Intel (1, 2, 5, and 10 Years)</h3><p>Markets are moving fast. Investors have already poured billions into semiconductor and AI infrastructure plays. But which companies stand to benefit the most?</p><p>Below is an updated stock price forecast table based on current valuations and credible analyst projections as of April 2025:</p><p>Stock Price Forecasts (as of April 2, 2025):</p><blockquote><p>ğŸ“Š â€œNVIDIA could become a $10 trillion company if AI growth keeps compoundingâ€”and the lights stay on.â€</p></blockquote><p>These projections reflect NVIDIAâ€™s dominant position in AI compute, AMDâ€™s momentum in data centers and HPC, and Intelâ€™s ambitious roadmap to regain relevance through AI chips, foundries, and quantum R&amp;D. All of them will need to align their compute strategies with scalable, clean energy sources to remain competitive in an energy-constrained world.</p><p>NVIDIA has become synonymous with AI hardware. Its data center businessâ€”driven by H100 GPUs, Grace Hopper CPUs, and DGX Cloudâ€”represents the backbone of the generative AI wave.</p><p>AMD is gaining momentum, fueled by its MI300 APUs and the integration of FPGA tech from its Xilinx acquisition. Its multi-pronged approachâ€”targeting AI, HPC, and data center marketsâ€”puts it in an ideal position to steal share.</p><p>Intel is the wildcard. Beset by delays in recent years, the company is now pursuing a bold strategy: reclaiming fabrication leadership, launching competitive AI accelerators (like Gaudi), and becoming a major contract chipmaker via Intel Foundry Services.</p><blockquote><p>ğŸ”® â€œIntel could be the greatest comeback story of the 2030sâ€”or a cautionary tale of missed revolutions.â€</p></blockquote><p>ğŸ”‹ Why AI Needs a New Kind of EnergyBy 2030, global energy demand from AI data centers is projected to add 3-4% to total global electricity use, according to Goldman Sachs. Thatâ€™s on top of already ballooning energy needs from cloud computing, blockchain, and high-performance computing.</p><p>The International Energy Agency (IEA) projects global data center electricity usage to double by 2026, reaching over 1,000 TWh.</p><blockquote><p>ğŸ“‰ â€œAIâ€™s growth isnâ€™t just constrained by chipsâ€”itâ€™s constrained by watts.â€</p></blockquote><p>Add to this the energy needs for cooling, especially in hot climates, and the infrastructure costs become even more complex. Many companies are racing to deploy liquid cooling, immersion tanks, and AI-optimized airflow systemsâ€”but all of that still requires a reliable and scalable energy supply.</p><p>Thatâ€™s where Japanâ€™s strategy gets interesting.</p><p>ğŸŒ Japanâ€™s Solar Gamble: The Rise of Perovskite PanelsAfter the Fukushima disaster in 2011, Japan reimagined its energy future. Once the global leader in solar panel manufacturing, it lost ground to subsidized Chinese firms. Now, Japan is betting on a comebackâ€”via perovskite solar cells (PSCs).</p><p>ğŸ—¾ â€œJapan lost the silicon solar race. It doesnâ€™t intend to lose the PSC one.â€</p><p>Perovskites offer several advantages:</p><p>Ultra-thin, lightweight, and semi-transparent</p><p>Capable of being printed on flexible substrates</p><p>Efficient even in low-light and indoor conditions</p><p>Potentially cheaper than silicon once scaled</p><p>With domestic production of iodine, a key ingredient in PSCs, Japan has a built-in supply chain advantage. The governmentâ€™s revised energy plan now prioritizes 20 GW of PSC-generated electricity by 2040.</p><p>ğŸŒ‡ â€œPSC tech is turning the walls, windows, and rooftops of our cities into power plants.â€</p><p>In March 2025, Japan unveiled the first commercial-scale PSC installation at the Osaka Expo site, covering nearly 2 kilometers of curved rooftop infrastructure. This real-world deployment is the starting line for broader urban PSC integration.</p><p>âš¡ â€œWith PSCs, Japan isnâ€™t just solving a climate problemâ€”itâ€™s laying the foundation for the AI age.â€</p><p>ğŸ¢ Powering the AI Data Center of the FutureImagine an NVIDIA supercomputer campus where the walls themselves generate power. Windows serve as photovoltaic glass. Rooftops are layered with flexible PSC film, and carports double as solar arrays.</p><p>This isnâ€™t hypotheticalâ€”itâ€™s a near-future design challenge being explored by major hyperscalers.</p><p>Already, Microsoft and Google are moving toward 24/7 carbon-free energy goals by 2030. NVIDIA, AWS, and Meta are investing in renewable energy PPAs, battery storage, and AI-driven grid optimization.</p><p>PSC technology enables localized generation in ways previously unimaginable:</p><p>Lower land use requirements</p><p>Better energy proximity (reducing transmission loss)</p><p>Potential hybridization with wind or geothermal</p><p>ğŸ”‹ â€œImagine NVIDIAâ€™s next-gen GPU clusters running entirely on solar-clad walls of the building theyâ€™re housed in.â€</p><p>This approach could become the norm in a world where carbon taxes, ESG requirements, and grid constraints collide with exponential AI demand.</p><p>ğŸ§  The Competitive Landscape: NVIDIA, AMD, Intel, and BeyondThe race to dominate AI compute is heating upâ€”and it's no longer just about clock speeds or core counts. The current landscape is shaped by three forces: the established semiconductor giants, the hyperscalers creating custom chips to reduce costs and dependence, and a new generation of startups focused on specialized AI workloads. Each group brings unique advantages, but all of them are being tested by one overarching constraint: power.</p><p>NVIDIA has earned its dominance through the unmatched performance of its GPUs, particularly the A100, H100, and upcoming Blackwell architecture. It controls over 80% of the AI accelerator market and maintains a massive software moat with its CUDA stack, which has become essential for developers. But NVIDIA isnâ€™t just a hardware companyâ€”itâ€™s evolving into a vertically integrated platform, offering full-stack infrastructure via DGX Cloud and actively developing tools like CUDA Quantum to prepare for a hybrid AI-quantum computing future.</p><p>ğŸ’¬ â€œNVIDIA isnâ€™t building quantum computersâ€”itâ€™s making sure itâ€™s indispensable when they arrive.â€</p><p>AMD has taken a different route, leaning into flexibility and performance-per-watt. Its MI300X GPUs and MI300A APUs blend CPU and GPU architectures to support both AI and HPC workloads. AMDâ€™s acquisition of Xilinx has given it access to FPGAs, which are critical for inference, telecom, and edge AI applications. AMD may not have NVIDIAâ€™s market share, but its power efficiency and adaptable designs position it well in an era where energy efficiency is paramount.</p><p>Intel, often dismissed in recent years, is quietly laying the groundwork for a potential comeback. Its Gaudi 3 chips are optimized for AI training at lower cost-per-performance ratios, and the Falcon Shores XPU represents a unified compute vision combining GPU and CPU. Intel Foundry Services (IFS) could become a geopolitical asset, as Western countries seek to secure chip manufacturing outside of East Asia. Furthermore, Intel continues to explore neuromorphic and quantum computing as long-term moonshots.</p><p>ğŸ”® â€œIntelâ€™s bet on national resilience and diversified compute might be its ace in the silicon war.â€</p><p>Beyond the traditional players, hyperscalers and cloud giants are becoming compute designers in their own right:</p><p>Googleâ€™s TPU v5 chips power internal workloads for DeepMind and Bard. Its in-house silicon strategy, coupled with quantum research at Google AI, positions it to leapfrog in areas like large-scale optimization.</p><p>Amazon builds its own chipsâ€”Inferentia and Trainiumâ€”to lower inference costs in AWS and improve scalability.</p><p>Microsoft partners with OpenAI while developing custom ARM-based Azure silicon to optimize its massive data center footprint.</p><p>Meta is designing its MTIA chips to train large language models more efficiently, reducing reliance on NVIDIA.</p><p>Tesla is building Dojo, a purpose-built supercomputer aimed at training AI models for self-driving vehicles.</p><p>A dynamic group of startups and disruptors are also redefining whatâ€™s possible:</p><p>Cerebrasâ€™ wafer-scale chips deliver massive memory and compute on a single slab of siliconâ€”ideal for enormous transformer models.</p><p>Graphcore and Groq focus on radically different architectures, prioritizing memory bandwidth and low-latency inference.</p><p>SambaNova builds vertically integrated AI systems, offering pre-trained models alongside custom hardware.</p><p>Lightmatter, Mythic, and Tenstorrent are pushing the boundaries with optical, analog, and RISC-V-based designs.</p><p>âš™ï¸ â€œThe future of AI hardware may look more like a constellation of specialized stars than one central sun.â€</p><p>Then, thereâ€™s the geopolitical layer. U.S. export controls on advanced chips to China have reshaped global supply chains. Legislation like the CHIPS Act (U.S.) and EU Chips Act are funneling billions into domestic semiconductor manufacturing. Taiwan and South Korea still dominate leading-edge fabrication, but Japan, India, and the U.S. are racing to catch upâ€”spurred by concerns over sovereignty and security.</p><p>This is not just a technology raceâ€”itâ€™s a global realignment. Winning in AI now requires more than better silicon. It demands:</p><p>Seamless integration between software and hardware ecosystems</p><p>Vertical control over infrastructure from chip design to cloud deployment</p><p>Robust and secure supply chains</p><p>And above all, energy-aware engineering across every layer of the stack</p><p>All players are now being judged not just on performance, but on how well they fit into a sustainable, scalable, and secure AI future.</p><p>The next 10 years wonâ€™t just be about faster chips. Theyâ€™ll be about how efficiently those chips can be powered, cooled, and operated.</p><p>PSC technology directly addresses this by enabling energy-aware infrastructure. Coupled with:</p><p>AI-optimized chip design (e.g. sparsity, quantization)</p><p>New cooling systems (e.g. direct-to-chip liquid loops)</p><p>Software for dynamic energy budgeting</p><p>The world is moving toward a future where every computation has an energy costâ€”and optimizing for that will be key.</p><p>ğŸŒ â€œAIâ€™s future isnâ€™t a straight lineâ€”itâ€™s a convergence of classical, quantum, and renewable infrastructure.â€</p><p>Even quantum computing, while not a near-term disruptor, will begin to supplement classical systems by 2030. NVIDIAâ€™s CUDA Quantum framework is already building the bridge.</p><p>ğŸ”® Looking to 2035: What the World Might Look LikeAs we look to 2035, the AI landscape will be shaped not just by progress in computation, but by how intelligently we integrate energy, compute, and infrastructure. The companies, cities, and nations that win will be those who master this convergence.</p><p>In the best-case scenario, we will have created a self-reinforcing ecosystem where AI helps optimize energy use, energy enables more AI compute, and both support increasingly autonomous and intelligent societies. Here's what that could look like:</p><p>PSC-integrated urban architecture: Buildings clad in perovskite solar cells generate clean electricity directly where itâ€™s neededâ€”on the rooftops and walls of data centers, offices, and residential towers. These structures no longer draw power from strained grids, but act as energy-positive nodes in smart urban environments.</p><p>Self-healing grids and AI-optimized infrastructure: Large language models (LLMs) and reinforcement learning agents trained on real-time grid data continuously monitor and optimize energy distribution, anticipating bottlenecks and redistributing loads. Blackouts become rare anomalies, and energy waste is virtually eliminated.</p><p>Modular data centers in emerging economies: With flexible PSC technology and compact cooling systems, developing nations deploy containerized AI data centers in remote regionsâ€”bringing edge compute capacity to areas previously off-grid. This unlocks education, healthcare, and climate research at global scale.</p><p>Quantum-classical hybrid clusters: Research universities and enterprise labs begin deploying true quantum-enhanced AI systems. Problems like protein folding, material discovery, and logistics optimization are solved in minutes, not years, with hybrid architectures powered by solar and cooled by seawater or AI-managed HVAC systems.</p><p>AI chipmakers as energy companies: NVIDIA, AMD, and Intel are no longer just semiconductor firmsâ€”theyâ€™re infrastructure companies. They build not only chips, but full-stack platforms bundled with energy solutions, vertical software, and on-premises AI-as-a-service offerings. Compute becomes a utility, priced per watt and delivered with sustainability guarantees.</p><p>ğŸŒŸ â€œIn the race to build smarter machines, data drives them, compute builds themâ€”but it's clean energy that will determine how far they go.â€</p><p>2035 could also be the tipping point for AGIâ€”if we have the compute and power to scale it safely. And that safety will be measured not just in algorithmic alignment, but in infrastructure sustainability, transparency, and resilience.</p><p>In short, the AI of 2035 wonâ€™t run in isolation. It will be shaped by the ecosystems we build around itâ€”biological, electrical, and societal.</p><p>The AI era will not be decided by raw performance alone. It will be decided by who can scale sustainably.</p><p>Japanâ€™s PSC initiative is one of the clearest indicators yet that nations are waking up to the energy side of AI. For chipmakers, this means aligning with renewables. For investors, it means tracking energy innovation as closely as chip roadmaps.</p><p>ğŸ” â€œAI isnâ€™t just about more computeâ€”itâ€™s about more sustainable compute.â€</p><p>In the coming decade, every AI breakthrough will cast a shadowâ€”measured not just in lines of code, but in kilowatt-hours.</p><p>The infrastructure of intelligence is being rewritten.</p><p>And itâ€™s powered by more than just silicon.</p><p>Itâ€™s powered by the sun.</p></div>
</body>
</html>